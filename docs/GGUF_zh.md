# Qwen3-VL GGUF èŠ‚ç‚¹æŒ‡å—

**Qwen3-VL GGUF (2B Thinking)** èŠ‚ç‚¹é€šè¿‡ [`llama-cpp-python`](https://github.com/ggerganov/llama.cpp/tree/master/bindings/python) è¿è¡Œ [`Qwen/Qwen3-VL-2B-Thinking-GGUF`](https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF)ï¼Œä¸“ä¸º CPU ä¸è½»é‡ GPU åœºæ™¯æ‰“é€ ã€‚

## ç¯å¢ƒè¦æ±‚
- å¿…é¡»ä»æºç ç¼–è¯‘ã€ä¸”å¯ç”¨äº† **MMVQ** çš„ `llama-cpp-python`ï¼ˆå®˜æ–¹ wheel æš‚æœªåŒ…å« Qwen3-VL è§†è§‰ç®—å­ï¼‰ã€‚è¯¦è§ [llama-cpp-python å®‰è£…ä¸ MMVQ ç¼–è¯‘æŒ‡å—](./LLAMA_CPP_INSTALL_zh.md)ã€‚
- Python 3.10+ï¼Œä»¥åŠå¯ç¼–è¯‘ llama.cpp çš„æœ¬åœ°å·¥å…·é“¾ï¼ˆLinux: GCC/Clangï¼ŒWindows: MSVCï¼‰ã€‚

## å®‰è£…æ­¥éª¤
1. å¸è½½ä¸å…¼å®¹çš„ wheelï¼š
   ```bash
   pip uninstall -y llama-cpp-python
   ```
2. é‡æ–°ç¼–è¯‘å¹¶å¯ç”¨ MMVQï¼š
   ```bash
   CMAKE_ARGS="-DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_MMVQ=ON" \
   pip install -U "llama-cpp-python @ git+https://github.com/ggerganov/llama.cpp.git#subdirectory=bindings/python"
   ```
3. ï¼ˆå¯é€‰ï¼‰éªŒè¯æ„å»ºä¿¡æ¯ï¼š
   ```bash
   python - <<'PY'
   from llama_cpp import Llama
   print("mmvq" in Llama.build_info().get("general", {}).get("features", {}).lower())
   PY
   ```

## ä½¿ç”¨æ–¹å¼
1. åœ¨ ğŸ§ªAILab/QwenVL åˆ†ç±»ä¸­æ·»åŠ  **Qwen3-VL GGUF (2B Thinking)** èŠ‚ç‚¹ã€‚
2. ä»ä¸‹æ‹‰èœå•é€‰æ‹©ä»»æ„ `.gguf` é‡åŒ–æ–‡ä»¶ï¼ˆèŠ‚ç‚¹ä¼šå®æ—¶è¯»å– Hugging Face çš„æ¸…å•ï¼‰ã€‚
3. é€‰æ‹©å›¾åƒï¼ˆå¯é€‰ï¼‰å¹¶è¾“å…¥æç¤ºè¯åè¿è¡Œå·¥ä½œæµã€‚
4. å¦‚éœ€å¤šæ¬¡å¤ç”¨åŒä¸€æ¨¡å‹ï¼Œè¯·å¯ç”¨ `keep_model_loaded` ä»¥é¿å…é‡å¤ä¸‹è½½ä¸åˆå§‹åŒ–ã€‚

èŠ‚ç‚¹ä¼šè‡ªåŠ¨è·å– `.gguf` ä»¥åŠåŒ¹é…çš„ projector æƒé‡ï¼Œç¼“å­˜åˆ° `models/LLM/Qwen-VL/GGUF/` ä¸‹ï¼Œå¹¶åŸºäº llama.cpp çš„ `qwen2_vl` æ¨¡æ¿æ‰§è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚

## å¸¸è§é—®é¢˜
- **â€œç¼ºå°‘ MMVQ Kernelâ€**ï¼šè¯·ä½¿ç”¨ä¸Šè¿°å‘½ä»¤é‡æ–°ç¼–è¯‘ `llama-cpp-python`ã€‚
- **CPU æ¨ç†è¿‡æ…¢**ï¼šå¯å°è¯•æ›´æ¿€è¿›çš„é‡åŒ–ï¼ˆå¦‚ `Q3_K_M`ï¼‰å¹¶ä¿æŒè¾ƒå°çš„ batchã€‚
- **æç¤º projector ä¸åŒ¹é…**ï¼šåˆ é™¤ç¼“å­˜çš„ `projector.bin`ï¼Œè®©èŠ‚ç‚¹é‡æ–°ä¸‹è½½å¯¹åº”æ–‡ä»¶ã€‚
